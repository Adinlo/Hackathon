{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d2c976",
   "metadata": {},
   "source": [
    "CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d7afa",
   "metadata": {},
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1033ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def cla_gen(lignes=10000, colonnes=10, nb_classes=30):\n",
    "\n",
    "    max_n_informative = min(colonnes, 10)\n",
    "    n_informative = min(colonnes, max_n_informative)\n",
    "    n_clusters_per_class = max(1, (colonnes // nb_classes) // 2)\n",
    "    \n",
    "    X, y = make_classification(\n",
    "        n_samples=lignes, \n",
    "        n_features=colonnes, \n",
    "        n_informative=n_informative, \n",
    "        n_redundant=colonnes - n_informative, \n",
    "        n_clusters_per_class=n_clusters_per_class, \n",
    "        n_classes=nb_classes\n",
    "    )\n",
    "\n",
    "    for i in range(colonnes):\n",
    "        X[:, i] = X[:, i] * np.random.randint(50, 150)\n",
    "    \n",
    "    df = pd.DataFrame(X, columns=[f\"Col{j}\" for j in range(1, X.shape[1]+1)])\n",
    "    df[\"y\"] = pd.Series(y, name=\"y\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Générateur de dataframe pour la regression\n",
    "def reg_gen(lignes=10000,colonnes=6):\n",
    "    temp = {}\n",
    "    y=0\n",
    "    \n",
    "    for i in range(colonnes):\n",
    "        temp[f\"X{i+1}\"] = np.random.rand(lignes) * np.random.randint(1,100)\n",
    "    \n",
    "    for i in temp.values():\n",
    "        y += i * np.random.randint(1,100)\n",
    "\n",
    "    y += np.random.randn(lignes)\n",
    "\n",
    "    df = pd.DataFrame(temp)\n",
    "    df[\"y\"] = pd.Series(y, name=\"y\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def tokenizator(serie):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "    batch_encoding = tokenizer(serie['review'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    series_dict = {key: pd.Series(value.numpy().flatten()) for key, value in batch_encoding.items()}\n",
    "    df_series = pd.DataFrame(series_dict)\n",
    "    df_series.iloc[:, 0]\n",
    "    return df_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7746f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tevals\tmin      \n",
      "0  \t1    \t0.0733469\n",
      "Generation 0: Best Individual = [10, 11, 0.9046171798997118, 0.07799573597025078, -0.4702711924999887, -0.6061063755437708, 0.27451379073873716, 0.8821002911229638, 0.7327237089466161, 0.782724001406528, -0.5640881312484347, 0.28337004313816494, 0.7596713380767055, 0.3975391891647746, -0.4961427468252737, 0.8152809479301126, -0.24125815275380869, 0.31825951215201975, 0.4977161402987891, 0.006218410042655442, -0.30128010310240616, 0.9791315363919884, 0.9796640438361728, -0.5898725559676765, -0.0958591226022465, -0.005227085397501607, -0.7285406791808369, 0.4504553802834663, 0.030667321094760558, -0.9061395573613995, 0.46311077026127445, 0.671044793370893, -0.3727026690194628, -0.014208869884899666, -0.01682317700722291, -0.21850378799843195, 0.04821496313118234, 0.22670105574091415, -0.14030592319945767, 0.30006280517783046, -0.44971217955916853, -0.6765905452681225, -0.4306242460751659, -0.3957092976855878, 0.3873348650159434, -0.05234379635059172, 0.21521766110754803, -0.41004062503049044, -0.820620959107391, -0.18407819733237685, 0.284784976957279, -0.7406352152889069, -0.3819258744780243, -0.6537660816954904, -0.8784207615671991, -0.15693045245955117, -0.7105507844927537, -0.5061541863859371, 0.35062255361424244, 0.5016460649048247, -0.74149400360155, -0.9739497571778901, 0.4295496825977243, 0.11374995846853309, -0.33682993409383744, 0.6095211514362846, 0.8247103548037997, 0.5541744310493473, 0.9701253090092989, -0.6379675724552978, -0.997507462023904, 0.8948361905350817, 0.15997863693510284, 0.7361134417237374, -0.2953421852895819, 0.9442236401220641, 0.27095232472015085, 0.1925498641228991, 0.9408776478071212, -0.8497921303942064, -0.20760053102131182, -0.8175559592718156, -0.8655386656727524, 0.45284885399925834, 0.771651539685906, -0.24157031131473694, -0.9041089809247067, 0.03368043127488285, 0.7127249444574808, -0.9325305300972762, 0.4996459125235393, -0.2705484740812447, 0.9837223798983104, -0.3059513670167262, -0.7991350010740677, -0.42751380145679674, 0.6630666447654727, 0.7895031091213855, -0.47451423580529517, -0.37670955340358336, -0.02858180281624323, -0.04996225898295115, -0.4230770770293941, 0.3145790967108002, -0.5558047962539345, 0.28902983465850607, -0.1296385980023802, -0.44008346759694916, 0.4889082874798536, -0.3068436788347364, -0.5518705123304497, -0.8234484485154063, -0.22054455675814344, -0.5678887377456352, -0.4546764155927223, 0.051510694692837644, -0.8787918692950825, -0.3165403264618607, -0.3721251694865717, -0.20505185236527956, -0.20979258409867718, 0.46605833654848694, -0.9311191980973024, 0.6025585507352775, -0.8217788083203479, -0.8242616426097225, -0.31844442845868937, 0.5405455481173844, 0.6075786793566607, 0.41075508091086554, 0.4485053655388729, -0.8033923104865148, 0.26187658928347224, 0.3459214934545145, 0.6563957821447646, 0.40432657756828294, -0.7860845486654988, -0.7721955476125673, 0.5553435081659814, -0.7939907855852206, -0.9920466425976266, 0.8896996121782494, -0.410292381152654, -0.4044036120027221, 0.33095972975936383, 0.050505390581102594, -0.5515758538124693, -0.30796342433804424, 0.20875977567393877, -0.14475853216883316, 0.8695681228825318, 0.7701220968349667]\n",
      "1  \t1    \t0.0733469\n",
      "Generation 1: Best Individual = [10, 11, 0.9046171798997118, 0.07799573597025078, -0.4702711924999887, -0.6061063755437708, 0.27451379073873716, 0.8821002911229638, 0.7327237089466161, 0.782724001406528, -0.5640881312484347, 0.28337004313816494, 0.7596713380767055, 0.3975391891647746, -0.4961427468252737, 0.8152809479301126, -0.24125815275380869, 0.31825951215201975, 0.4977161402987891, 0.006218410042655442, -0.30128010310240616, 0.9791315363919884, 0.9796640438361728, -0.5898725559676765, -0.0958591226022465, -0.005227085397501607, -0.7285406791808369, 0.4504553802834663, 0.030667321094760558, -0.9061395573613995, 0.46311077026127445, 0.671044793370893, -0.3727026690194628, -0.014208869884899666, -0.01682317700722291, -0.21850378799843195, 0.04821496313118234, 0.22670105574091415, -0.14030592319945767, 0.30006280517783046, -0.44971217955916853, -0.6765905452681225, -0.4306242460751659, -0.3957092976855878, 0.3873348650159434, -0.05234379635059172, 0.21521766110754803, -0.41004062503049044, -0.820620959107391, -0.18407819733237685, 0.284784976957279, -0.7406352152889069, -0.3819258744780243, -0.6537660816954904, -0.8784207615671991, -0.15693045245955117, -0.7105507844927537, -0.5061541863859371, 0.35062255361424244, 0.5016460649048247, -0.74149400360155, -0.9739497571778901, 0.4295496825977243, 0.11374995846853309, -0.33682993409383744, 0.6095211514362846, 0.8247103548037997, 0.5541744310493473, 0.9701253090092989, -0.6379675724552978, -0.997507462023904, 0.8948361905350817, 0.15997863693510284, 0.7361134417237374, -0.2953421852895819, 0.9442236401220641, 0.27095232472015085, 0.1925498641228991, 0.9408776478071212, -0.8497921303942064, -0.20760053102131182, -0.8175559592718156, -0.8655386656727524, 0.45284885399925834, 0.771651539685906, -0.24157031131473694, -0.9041089809247067, 0.03368043127488285, 0.7127249444574808, -0.9325305300972762, 0.4996459125235393, -0.2705484740812447, 0.9837223798983104, -0.3059513670167262, -0.7991350010740677, -0.42751380145679674, 0.6630666447654727, 0.7895031091213855, -0.47451423580529517, -0.37670955340358336, -0.02858180281624323, -0.04996225898295115, -0.4230770770293941, 0.3145790967108002, -0.5558047962539345, 0.28902983465850607, -0.1296385980023802, -0.44008346759694916, 0.4889082874798536, -0.3068436788347364, -0.5518705123304497, -0.8234484485154063, -0.22054455675814344, -0.5678887377456352, -0.4546764155927223, 0.051510694692837644, -0.8787918692950825, -0.3165403264618607, -0.3721251694865717, -0.20505185236527956, -0.20979258409867718, 0.46605833654848694, -0.9311191980973024, 0.6025585507352775, -0.8217788083203479, -0.8242616426097225, -0.31844442845868937, 0.5405455481173844, 0.6075786793566607, 0.41075508091086554, 0.4485053655388729, -0.8033923104865148, 0.26187658928347224, 0.3459214934545145, 0.6563957821447646, 0.40432657756828294, -0.7860845486654988, -0.7721955476125673, 0.5553435081659814, -0.7939907855852206, -0.9920466425976266, 0.8896996121782494, -0.410292381152654, -0.4044036120027221, 0.33095972975936383, 0.050505390581102594, -0.5515758538124693, -0.30796342433804424, 0.20875977567393877, -0.14475853216883316, 0.8695681228825318, 0.7701220968349667]\n",
      "2  \t0    \t0.0354464\n",
      "Generation 2: Best Individual = [10, 11, 0.9046171798997118, 0.07799573597025078, -0.4702711924999887, -0.6061063755437708, 0.27451379073873716, 0.8821002911229638, 0.7327237089466161, 0.782724001406528, -0.5640881312484347, 0.28337004313816494, 0.7596713380767055, 0.3975391891647746, -0.4961427468252737, 0.8152809479301126, -0.24125815275380869, 0.31825951215201975, 0.4977161402987891, 0.006218410042655442, -0.30128010310240616, 0.9791315363919884, 0.9796640438361728, -0.5898725559676765, -0.0958591226022465, -0.005227085397501607, -0.7285406791808369, 0.4504553802834663, 0.030667321094760558, -0.9061395573613995, 0.46311077026127445, 0.671044793370893, -0.3727026690194628, -0.014208869884899666, -0.01682317700722291, -0.21850378799843195, 0.04821496313118234, 0.22670105574091415, -0.14030592319945767, 0.30006280517783046, -0.44971217955916853, -0.6765905452681225, -0.4306242460751659, -0.3957092976855878, 0.3873348650159434, -0.05234379635059172, 0.21521766110754803, -0.41004062503049044, -0.820620959107391, -0.18407819733237685, 0.284784976957279, -0.7406352152889069, -0.3819258744780243, -0.6537660816954904, -0.8784207615671991, -0.15693045245955117, -0.7105507844927537, -0.5061541863859371, 0.35062255361424244, 0.5016460649048247, -0.74149400360155, -0.9739497571778901, 0.4295496825977243, 0.11374995846853309, -0.33682993409383744, 0.6095211514362846, 0.8247103548037997, 0.5541744310493473, 0.9701253090092989, -0.6379675724552978, -0.997507462023904, 0.8948361905350817, 0.15997863693510284, 0.7361134417237374, -0.2953421852895819, 0.9442236401220641, 0.27095232472015085, 0.1925498641228991, 0.9408776478071212, -0.8497921303942064, -0.20760053102131182, -0.8175559592718156, -0.8655386656727524, 0.45284885399925834, 0.771651539685906, -0.24157031131473694, -0.9041089809247067, 0.03368043127488285, 0.7127249444574808, -0.9325305300972762, 0.4996459125235393, -0.2705484740812447, 0.9837223798983104, -0.3059513670167262, -0.7991350010740677, -0.42751380145679674, 0.6630666447654727, 0.7895031091213855, -0.47451423580529517, -0.37670955340358336, -0.02858180281624323, -0.04996225898295115, -0.4230770770293941, 0.3145790967108002, -0.5558047962539345, 0.28902983465850607, -0.1296385980023802, -0.44008346759694916, 0.4889082874798536, -0.3068436788347364, -0.5518705123304497, -0.8234484485154063, -0.22054455675814344, -0.5678887377456352, -0.4546764155927223, 0.051510694692837644, -0.8787918692950825, -0.3165403264618607, -0.3721251694865717, -0.20505185236527956, -0.20979258409867718, 0.46605833654848694, -0.9311191980973024, 0.6025585507352775, -0.8217788083203479, -0.8242616426097225, -0.31844442845868937, 0.5405455481173844, 0.6075786793566607, 0.41075508091086554, 0.4485053655388729, -0.8033923104865148, 0.26187658928347224, 0.3459214934545145, 0.6563957821447646, 0.40432657756828294, -0.7860845486654988, -0.7721955476125673, 0.5553435081659814, -0.7939907855852206, -0.9920466425976266, 0.8896996121782494, -0.410292381152654, -0.4044036120027221, 0.33095972975936383, 0.050505390581102594, -0.5515758538124693, -0.30796342433804424, 0.20875977567393877, -0.14475853216883316, 0.8695681228825318, 0.7701220968349667]\n",
      "3  \t1    \t0.026384 \n",
      "Generation 3: Best Individual = [10, 11, -0.22590495777696562, -0.05227893253139304, 0.3030558726981736, 0.49098797337288413, -0.7324947012554501, 0.3194187953670562, -0.32767925768829276, -0.2907152682866996, 0.7478530192110111, 0.40154226277215055, 0.7810886272467212, -0.5298214474196521, -0.18634188142774866, 0.3107199278723225, -0.13266487710349129, -0.4289210240125947, 0.9913003679276275, 0.3250698309960809, -0.6750511683280485, 0.17741232580116595, -0.999046726581077, 0.2592829114265247, -0.33774829830313835, -0.6446928345383398, -0.4638015883813229, -0.8479561768554775, 0.3347744371645731, -0.8250535670369874, 0.7581802375972058, -0.21432937075579783, 0.49713605008958917, -0.7715698741867076, 0.5878786525209303, 0.3767597136025673, -0.5106879700073068, 0.016860540324346474, -0.9906109830003078, -0.6187948173475395, -0.03753937270373631, -0.520733074187979, 0.27247613815066773, 0.9145490660643212, 0.44403034712705614, 0.8553892159963208, 0.5338978429226906, 0.6269361425925204, 0.47944703915758247, 0.4148848969481862, 0.10062344938733414, 0.34753584082649014, -0.7545404042584263, -0.4635703329702827, 0.11020220842443096, -0.31839500926640607, -0.17235223766303243, -0.5427190973442371, -0.8732936335997494, 0.7348558615086742, 0.3662180318788606, 0.1221530528666055, 0.8146177482594295, -0.024249428315992194, 0.6751397541825193, -0.6233326474737642, 0.036003865762310516, -0.08566668084745421, -0.5642956556026237, -0.3896790132710328, 0.005519305854062129, -0.3990742745583118, 0.7428679954007409, 0.06181060757059287, 0.1778625253829922, 0.7476856935089928, 0.43071966612999946, 0.41586018985321904, -0.8995166198587969, 0.5846902583703173, -0.8941328940732747, -0.24734803354390822, 0.9571694700986977, 0.031469493141524474, -0.7934680573203534, -0.9519391995941122, 0.9122122312150238, -0.936593203578322, 0.5787519378887243, -0.03409358520828998, -0.564000353753372, -0.5545918618581933, -0.29235457449825786, -0.20919748896038626, 0.0795414368229741, -0.8039322209600865, -0.7355990873970555, -0.19515295544329914, -0.8931473485502095, -0.39444380575336724, -0.46340769506011514, 0.5773203233020516, -0.9870595055397389, 0.8845000591836987, -0.9849399536511807, -0.5385756453109296, 0.6515976942431116, -0.4020567218551716, 0.7952735148204015, -0.9173728529201346, -0.09207310327223506, -0.5077912808959255, 0.6972920598103847, -0.40895008553100465, 0.5250409218148278, 0.4560211184826741, -0.20505919024068708, -0.8590661519888572, 0.41125837133166354, -0.5038978832239487, -0.6680916098446026, -0.8155901479771446, -0.4169756600304668, -0.7249158477801827, -0.3828786454055926, -0.20242482647630822, 0.6413264225752504, -0.3115254620742678, -0.06347311782613274, -0.27925310243416357, 0.7494187841982847, -0.29272149711655504, 0.7140445017438031, -0.8792375876601037, 0.6878392929746309, 0.7287959528406158, -0.6396625517964349, -0.589877007946813, -0.37422119838244705, 0.6237506519019074, 0.9174526834073318, 0.7992149569475409, 0.34196592581161034, -0.8804480743253424, 0.29384509495776623, -0.5132811241889457, -0.6569668642759665, 0.64530471918038, -0.6886896240782294, 0.2885133027294544, 0.9724935423302583, 0.3953153531717546]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from deap import base, creator, tools\n",
    "import tensorflow as tf\n",
    "\n",
    "# Charger les données depuis un fichier CSV\n",
    "def load_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    X = data.iloc[:, :-1].values  # Toutes les colonnes sauf la dernière sont des caractéristiques\n",
    "    y = data.iloc[:, -1].values  # La dernière colonne est la cible\n",
    "    return X, y\n",
    "\n",
    "# Prétraiter les données\n",
    "def preprocess_data(X, y):\n",
    "    if y.dtype == 'object' or y.dtype == 'str':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    y = to_categorical(y)  # Conversion en one-hot encoding\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, X.shape[1], y.shape[1]\n",
    "\n",
    "# Créer le toolbox pour les individus\n",
    "def create_individual_toolbox(input_dim, output_dim):\n",
    "    if 'FitnessMin' not in creator.__dict__:\n",
    "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "    if 'Individual' not in creator.__dict__:\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "    \n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_int\", np.random.randint, 5, 12)\n",
    "    toolbox.register(\"attr_float\", np.random.uniform, -1, 1)\n",
    "    \n",
    "    def generate_individual(num_layers=None):\n",
    "        if num_layers is None:\n",
    "            num_layers = np.random.randint(1, 5)  # Nombre de couches entre 1 et 4\n",
    "        structure = [np.random.randint(5, 12) for _ in range(num_layers)]\n",
    "        total_weights = sum([structure[i-1] * structure[i] if i > 0 else input_dim * structure[i] for i in range(num_layers)])\n",
    "        weights = [toolbox.attr_float() for _ in range(total_weights)]\n",
    "        individual = structure + weights\n",
    "        return creator.Individual(individual)\n",
    "    \n",
    "    def generate_diverse_population(n):\n",
    "        population = []\n",
    "        seen_structures = set()\n",
    "        while len(population) < n:\n",
    "            num_layers = np.random.randint(1, 5)\n",
    "            individual = generate_individual(num_layers=num_layers)\n",
    "            structure, _ = decode_individual(individual)\n",
    "            structure_tuple = tuple(structure)\n",
    "            if structure_tuple not in seen_structures:\n",
    "                population.append(individual)\n",
    "                seen_structures.add(structure_tuple)\n",
    "        return population\n",
    "\n",
    "    toolbox.register(\"individual\", generate_individual)\n",
    "    toolbox.register(\"population\", generate_diverse_population)\n",
    "\n",
    "    return toolbox\n",
    "\n",
    "# Fonction pour extraire la structure et les poids d'un individu\n",
    "def decode_individual(individual):\n",
    "    num_layers = len([x for x in individual if isinstance(x, int)])\n",
    "    structure = individual[:num_layers]\n",
    "    weights = individual[num_layers:]\n",
    "    return structure, weights\n",
    "\n",
    "# Déplacer la définition de la fonction eval_nn ici\n",
    "def eval_nn(individual, input_dim, output_dim, X_train, y_train, X_test, y_test):\n",
    "    structure, weights = decode_individual(individual)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "    total_neurons = 0\n",
    "    weight_idx = 0\n",
    "    for neurons in structure:\n",
    "        if not isinstance(neurons, int):\n",
    "            neurons = int(neurons)\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        total_neurons += neurons\n",
    "        output_dim_layer = neurons\n",
    "        if weight_idx + input_dim * output_dim_layer <= len(weights):\n",
    "            layer_weights = np.array(weights[weight_idx:weight_idx + input_dim * output_dim_layer]).reshape(input_dim, output_dim_layer)\n",
    "            weight_idx += input_dim * output_dim_layer\n",
    "            input_dim = output_dim_layer\n",
    "        else:\n",
    "            return (float('inf'),), 0, float('inf'), [], [], [], []  # Eviter les erreurs d'index\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "    \n",
    "    weight_idx = 0\n",
    "    input_dim = X_train.shape[1]\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Dense):\n",
    "            output_dim_layer = layer.units\n",
    "            if weight_idx + input_dim * output_dim_layer <= len(weights):\n",
    "                layer_weights = np.array(weights[weight_idx:weight_idx + input_dim * output_dim_layer]).reshape(input_dim, output_dim_layer)\n",
    "                weight_idx += input_dim * output_dim_layer\n",
    "                layer.set_weights([layer_weights, np.zeros(output_dim_layer)])\n",
    "                input_dim = output_dim_layer\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=20, verbose=0, validation_data=(X_test, y_test))\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return (loss,), accuracy, loss, history.history['accuracy'], history.history['val_accuracy'], history.history['loss'], history.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "# Mutation des individus\n",
    "def mutate_individual(individual):\n",
    "    structure, weights = decode_individual(individual)\n",
    "    if np.random.random() < 0.5:\n",
    "        for i in range(len(structure)):\n",
    "            if np.random.random() < 0.2:\n",
    "                structure[i] = np.random.randint(5, 12)\n",
    "    if np.random.random() < 0.5:\n",
    "        weights = tools.mutPolynomialBounded(weights, low=-1, up=1, eta=0.1, indpb=0.2)[0]\n",
    "    individual[:] = structure + weights\n",
    "    return individual,\n",
    "\n",
    "# Exécution de l'algorithme génétique\n",
    "def run_genetic_algorithm(filepath, n_population=5, n_generations=10):\n",
    "    if isinstance(filepath, str):\n",
    "        X, y = load_data(filepath)\n",
    "    else:\n",
    "        X = filepath\n",
    "        y = X[X.columns[-1]]\n",
    "        X.drop(X.columns[-1], axis=1, inplace=True)\n",
    "        for column in X.columns:\n",
    "            if X[column].dtype == 'object' or X[column].dtype == 'str':\n",
    "                X[column] = tokenizator(X[column])\n",
    "    X_train, X_test, y_train, y_test, input_dim, output_dim = preprocess_data(X, y)\n",
    "    \n",
    "    toolbox = create_individual_toolbox(input_dim, output_dim)\n",
    "    toolbox.register(\"evaluate\", eval_nn, input_dim=input_dim, output_dim=output_dim, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", mutate_individual)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    \n",
    "    population = toolbox.population(n=n_population)\n",
    "    \n",
    "    # Évaluer tous les individus de la population initiale\n",
    "    fitnesses = list(map(toolbox.evaluate, population))\n",
    "    for ind, fit in zip(population, fitnesses):\n",
    "        ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "        ind.accuracy = fit[1]\n",
    "        ind.raw_loss = fit[2]\n",
    "        ind.train_accuracy = fit[3]\n",
    "        ind.val_accuracy = fit[4]\n",
    "        ind.train_loss = fit[5]\n",
    "        ind.val_loss = fit[6]\n",
    "    \n",
    "    hall_of_fame = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", lambda values: np.min([val[0] for val in values]))\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = [\"gen\", \"evals\"] + stats.fields\n",
    "    \n",
    "    best_accuracy_per_gen = []\n",
    "    best_loss_per_gen = []\n",
    "    \n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for gen in range(n_generations):\n",
    "        # Garder le meilleur individu de la génération précédente\n",
    "        if hall_of_fame:\n",
    "            population.append(toolbox.clone(hall_of_fame[0]))\n",
    "        \n",
    "        # Sélectionner les meilleurs individus\n",
    "        best_individuals = tools.selBest(population, k=int(0.1 * len(population)))\n",
    "        # Sélectionner quelques mauvais individus\n",
    "        worst_individuals = tools.selWorst(population, k=int(0.1 * len(population)))\n",
    "        # Sélectionner les individus restants\n",
    "        remaining_individuals = tools.selTournament(population, k=len(population) - len(best_individuals) - len(worst_individuals), tournsize=3)\n",
    "        \n",
    "        offspring = best_individuals + worst_individuals + remaining_individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if np.random.random() < 0.5:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        for mutant in offspring:\n",
    "            if np.random.random() < 0.2:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "        \n",
    "        # Évaluer les individus invalides\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "            ind.accuracy = fit[1]\n",
    "            ind.raw_loss = fit[2]\n",
    "            ind.train_accuracy = fit[3]\n",
    "            ind.val_accuracy = fit[4]\n",
    "            ind.train_loss = fit[5]\n",
    "            ind.val_loss = fit[6]\n",
    "        \n",
    "        # Ajouter quelques nouveaux individus aléatoires pour la diversité\n",
    "        new_random_individuals = [toolbox.individual(num_layers=np.random.randint(1, 5)) for _ in range(int(0.1 * len(population)))]\n",
    "        population[:] = offspring + new_random_individuals\n",
    "        \n",
    "        # S'assurer qu'il y a des individus avec 1 et 2 couches\n",
    "        has_one_layer = any(len(decode_individual(ind)[0]) == 1 for ind in population)\n",
    "        has_two_layers = any(len(decode_individual(ind)[0]) == 2 for ind in population)\n",
    "        if not has_one_layer:\n",
    "            population.append(toolbox.individual(num_layers=1))\n",
    "        if not has_two_layers:\n",
    "            population.append(toolbox.individual(num_layers=2))\n",
    "        \n",
    "        # S'assurer que tous les individus ont des valeurs de fitness valides\n",
    "        invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "        if invalid_ind:\n",
    "            fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "            for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "                ind.accuracy = fit[1]\n",
    "                ind.raw_loss = fit[2]\n",
    "                ind.train_accuracy = fit[3]\n",
    "                ind.val_accuracy = fit[4]\n",
    "                ind.train_loss = fit[5]\n",
    "                ind.val_loss = fit[6]\n",
    "        \n",
    "        # Enregistrer les statistiques\n",
    "        record = stats.compile(population)\n",
    "        logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
    "        print(logbook.stream)\n",
    "        \n",
    "        hall_of_fame.update(population)\n",
    "        \n",
    "        best_ind = tools.selBest(population, 1)[0]\n",
    "        best_accuracy_per_gen.append(best_ind.accuracy)\n",
    "        best_loss_per_gen.append(best_ind.fitness.values[0])\n",
    "    \n",
    "        train_accuracies.append(best_ind.train_accuracy)\n",
    "        val_accuracies.append(best_ind.val_accuracy)\n",
    "        train_losses.append(best_ind.train_loss)\n",
    "        val_losses.append(best_ind.val_loss)\n",
    "\n",
    "        # Afficher le meilleur individu de chaque génération\n",
    "        print(f\"Generation {gen}: Best Individual = {best_ind}\")\n",
    "        \n",
    "        # Arrêter si un modèle avec 1 couche atteint une accuracy de 98%\n",
    "        if best_ind.accuracy >= 0.98 and len(decode_individual(best_ind)[0]) == 1:\n",
    "            print(\"Stopping early as a single layer model reached 98% accuracy.\")\n",
    "            break\n",
    "\n",
    "    # Affichage du meilleur individu\n",
    "    best_individual = hall_of_fame[0]\n",
    "    display_best_individual(best_individual, X_train.shape[1])\n",
    "\n",
    "    best_structure, best_weights = decode_individual(best_individual)\n",
    "    print('Best Individual Structure:', best_structure)\n",
    "    print('Fitness (Loss, Num Layers, Total Neurons):', best_individual.fitness.values)\n",
    "    print('Accuracy of the Best Model:', best_individual.accuracy)\n",
    "    print('Raw Loss:', best_individual.raw_loss)\n",
    "    \n",
    "    # Calcul de l'accuracy du meilleur modèle au cours des époques\n",
    "    best_model_layers = best_structure\n",
    "    best_model = Sequential()\n",
    "    best_model.add(Input(shape=(X_train.shape[1],)))\n",
    "    for neurons in best_model_layers:\n",
    "        best_model.add(Dense(neurons, activation='relu'))\n",
    "    best_model.add(Dense(output_dim, activation='softmax'))\n",
    "    best_model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "    history = best_model.fit(X_train, y_train, epochs=20, verbose=0, validation_data=(X_test, y_test))\n",
    "    loss, accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Final Accuracy of the Best Model:', accuracy)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs for Best Model')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over Epochs for Best Model')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(best_loss_per_gen, label='Best Loss per Generation')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Best Loss per Generation')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(best_accuracy_per_gen, label='Best Accuracy per Generation')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Best Accuracy per Generation')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Fonction pour afficher la structure et les poids du meilleur individu\n",
    "def display_best_individual(best_individual, input_dim):\n",
    "    structure, weights = decode_individual(best_individual)\n",
    "    \n",
    "    print(\"Best Individual Structure:\")\n",
    "    print(structure)\n",
    "    \n",
    "    print(\"\\nBest Individual Weights:\")\n",
    "    weight_idx = 0\n",
    "    input_dim_layer = input_dim\n",
    "    for i, neurons in enumerate(structure):\n",
    "        output_dim_layer = neurons\n",
    "        layer_weights = np.array(weights[weight_idx:weight_idx + input_dim_layer * output_dim_layer]).reshape(input_dim_layer, output_dim_layer)\n",
    "        print(f\"Layer {i + 1} - Weights:\\n{layer_weights}\")\n",
    "        weight_idx += input_dim_layer * output_dim_layer\n",
    "        input_dim_layer = output_dim_layer\n",
    "\n",
    "    print(\"\\nBest Individual Biases:\")\n",
    "    for i, neurons in enumerate(structure):\n",
    "        biases = weights[weight_idx:weight_idx + neurons]\n",
    "        print(f\"Layer {i + 1} - Biases:\\n{biases}\")\n",
    "        weight_idx += neurons\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Utilisation de la fonction pour enregistrer les données Iris dans un fichier CSV\n",
    "filepath = cla_gen(lignes = 10000, colonnes = 10, nb_classes = 30)\n",
    "run_genetic_algorithm(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc0f4b",
   "metadata": {},
   "source": [
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca79e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Fonction pour télécharger et préparer les données wine\n",
    "def download_wine_data(filepath):\n",
    "    wine = load_wine()\n",
    "    df = pd.DataFrame(data=np.c_[wine['data'], wine['target']], columns=wine['feature_names'] + ['target'])\n",
    "    df.to_csv(filepath, index=False)\n",
    "\n",
    "# Utilisation de la fonction pour enregistrer les données wine dans un fichier CSV\n",
    "filepath = 'wine.csv'\n",
    "download_wine_data(filepath)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Charger les données depuis un fichier CSV\n",
    "def load_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    X = data.iloc[:, :-1].values  # Toutes les colonnes sauf la dernière sont des caractéristiques\n",
    "    y = data.iloc[:, -1].values  # La dernière colonne est la cible\n",
    "    return X, y\n",
    "\n",
    "# Prétraiter les données\n",
    "def preprocess_data(X, y):\n",
    "    if y.dtype == 'object' or y.dtype == 'str':\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    y = to_categorical(y)  # Conversion en one-hot encoding\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, X.shape[1], y.shape[1]\n",
    "\n",
    "# Créer le toolbox pour les individus\n",
    "def create_individual_toolbox(input_dim, output_dim):\n",
    "    if 'FitnessMin' not in creator.__dict__:\n",
    "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "    if 'Individual' not in creator.__dict__:\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "    \n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_int\", np.random.randint, 5, 12)\n",
    "    toolbox.register(\"attr_float\", np.random.uniform, -1, 1)\n",
    "    \n",
    "    def generate_individual(num_layers=None):\n",
    "        if num_layers is None:\n",
    "            num_layers = np.random.randint(1, 5)  # Nombre de couches entre 1 et 4\n",
    "        structure = [np.random.randint(5, 12) for _ in range(num_layers)]\n",
    "        total_weights = sum([structure[i-1] * structure[i] if i > 0 else input_dim * structure[i] for i in range(num_layers)])\n",
    "        weights = [toolbox.attr_float() for _ in range(total_weights)]\n",
    "        individual = structure + weights\n",
    "        return creator.Individual(individual)\n",
    "    \n",
    "    def generate_diverse_population(n):\n",
    "        population = []\n",
    "        seen_structures = set()\n",
    "        while len(population) < n:\n",
    "            num_layers = np.random.randint(1, 5)\n",
    "            individual = generate_individual(num_layers=num_layers)\n",
    "            structure, _ = decode_individual(individual)\n",
    "            structure_tuple = tuple(structure)\n",
    "            if structure_tuple not in seen_structures:\n",
    "                population.append(individual)\n",
    "                seen_structures.add(structure_tuple)\n",
    "        return population\n",
    "\n",
    "    toolbox.register(\"individual\", generate_individual)\n",
    "    toolbox.register(\"population\", generate_diverse_population)\n",
    "\n",
    "    return toolbox\n",
    "\n",
    "# Fonction pour extraire la structure et les poids d'un individu\n",
    "def decode_individual(individual):\n",
    "    num_layers = len([x for x in individual if isinstance(x, int)])\n",
    "    structure = individual[:num_layers]\n",
    "    weights = individual[num_layers:]\n",
    "    return structure, weights\n",
    "\n",
    "# Déplacer la définition de la fonction eval_nn ici\n",
    "def eval_nn(individual, input_dim, output_dim, X_train, y_train, X_test, y_test):\n",
    "    structure, weights = decode_individual(individual)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "    total_neurons = 0\n",
    "    weight_idx = 0\n",
    "    for neurons in structure:\n",
    "        if not isinstance(neurons, int):\n",
    "            neurons = int(neurons)\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        total_neurons += neurons\n",
    "        output_dim_layer = neurons\n",
    "        if weight_idx + input_dim * output_dim_layer <= len(weights):\n",
    "            layer_weights = np.array(weights[weight_idx:weight_idx + input_dim * output_dim_layer]).reshape(input_dim, output_dim_layer)\n",
    "            weight_idx += input_dim * output_dim_layer\n",
    "            input_dim = output_dim_layer\n",
    "        else:\n",
    "            return (float('inf'),), 0, float('inf'), [], [], [], []  # Eviter les erreurs d'index\n",
    "    model.add(Dense(output_dim, activation='softmax'))\n",
    "    \n",
    "    weight_idx = 0\n",
    "    input_dim = X_train.shape[1]\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Dense):\n",
    "            output_dim_layer = layer.units\n",
    "            if weight_idx + input_dim * output_dim_layer <= len(weights):\n",
    "                layer_weights = np.array(weights[weight_idx:weight_idx + input_dim * output_dim_layer]).reshape(input_dim, output_dim_layer)\n",
    "                weight_idx += input_dim * output_dim_layer\n",
    "                layer.set_weights([layer_weights, np.zeros(output_dim_layer)])\n",
    "                input_dim = output_dim_layer\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, epochs=20, verbose=0, validation_data=(X_test, y_test))\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return (loss,), accuracy, loss, history.history['accuracy'], history.history['val_accuracy'], history.history['loss'], history.history['val_loss']\n",
    "\n",
    "\n",
    "\n",
    "# Mutation des individus\n",
    "def mutate_individual(individual):\n",
    "    structure, weights = decode_individual(individual)\n",
    "    if np.random.random() < 0.5:\n",
    "        for i in range(len(structure)):\n",
    "            if np.random.random() < 0.2:\n",
    "                structure[i] = np.random.randint(5, 12)\n",
    "    if np.random.random() < 0.5:\n",
    "        weights = tools.mutPolynomialBounded(weights, low=-1, up=1, eta=0.1, indpb=0.2)[0]\n",
    "    individual[:] = structure + weights\n",
    "    return individual,\n",
    "\n",
    "# Exécution de l'algorithme génétique\n",
    "def run_genetic_algorithm(filepath, n_population=5, n_generations=10):\n",
    "    if isinstance(filepath, str):\n",
    "        X, y = load_data(filepath)\n",
    "    else:\n",
    "        X = filepath\n",
    "        y = X[X.columns[-1]]\n",
    "        X.drop(X.columns[-1], axis=1, inplace=True)\n",
    "        for column in X.columns:\n",
    "            if X[column].dtype == 'object' or X[column].dtype == 'str':\n",
    "                X[column] = tokenizator(X[column])\n",
    "    X_train, X_test, y_train, y_test, input_dim, output_dim = preprocess_data(X, y)\n",
    "    \n",
    "    toolbox = create_individual_toolbox(input_dim, output_dim)\n",
    "    toolbox.register(\"evaluate\", eval_nn, input_dim=input_dim, output_dim=output_dim, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", mutate_individual)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    \n",
    "    population = toolbox.population(n=n_population)\n",
    "    \n",
    "    # Évaluer tous les individus de la population initiale\n",
    "    fitnesses = list(map(toolbox.evaluate, population))\n",
    "    for ind, fit in zip(population, fitnesses):\n",
    "        ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "        ind.accuracy = fit[1]\n",
    "        ind.raw_loss = fit[2]\n",
    "        ind.train_accuracy = fit[3]\n",
    "        ind.val_accuracy = fit[4]\n",
    "        ind.train_loss = fit[5]\n",
    "        ind.val_loss = fit[6]\n",
    "    \n",
    "    hall_of_fame = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", lambda values: np.min([val[0] for val in values]))\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = [\"gen\", \"evals\"] + stats.fields\n",
    "    \n",
    "    best_accuracy_per_gen = []\n",
    "    best_loss_per_gen = []\n",
    "    \n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for gen in range(n_generations):\n",
    "        # Garder le meilleur individu de la génération précédente\n",
    "        if hall_of_fame:\n",
    "            population.append(toolbox.clone(hall_of_fame[0]))\n",
    "        \n",
    "        # Sélectionner les meilleurs individus\n",
    "        best_individuals = tools.selBest(population, k=int(0.1 * len(population)))\n",
    "        # Sélectionner quelques mauvais individus\n",
    "        worst_individuals = tools.selWorst(population, k=int(0.1 * len(population)))\n",
    "        # Sélectionner les individus restants\n",
    "        remaining_individuals = tools.selTournament(population, k=len(population) - len(best_individuals) - len(worst_individuals), tournsize=3)\n",
    "        \n",
    "        offspring = best_individuals + worst_individuals + remaining_individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if np.random.random() < 0.5:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        for mutant in offspring:\n",
    "            if np.random.random() < 0.2:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "        \n",
    "        # Évaluer les individus invalides\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "            ind.accuracy = fit[1]\n",
    "            ind.raw_loss = fit[2]\n",
    "            ind.train_accuracy = fit[3]\n",
    "            ind.val_accuracy = fit[4]\n",
    "            ind.train_loss = fit[5]\n",
    "            ind.val_loss = fit[6]\n",
    "        \n",
    "        # Ajouter quelques nouveaux individus aléatoires pour la diversité\n",
    "        new_random_individuals = [toolbox.individual(num_layers=np.random.randint(1, 5)) for _ in range(int(0.1 * len(population)))]\n",
    "        population[:] = offspring + new_random_individuals\n",
    "        \n",
    "        # S'assurer qu'il y a des individus avec 1 et 2 couches\n",
    "        has_one_layer = any(len(decode_individual(ind)[0]) == 1 for ind in population)\n",
    "        has_two_layers = any(len(decode_individual(ind)[0]) == 2 for ind in population)\n",
    "        if not has_one_layer:\n",
    "            population.append(toolbox.individual(num_layers=1))\n",
    "        if not has_two_layers:\n",
    "            population.append(toolbox.individual(num_layers=2))\n",
    "        \n",
    "        # S'assurer que tous les individus ont des valeurs de fitness valides\n",
    "        invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "        if invalid_ind:\n",
    "            fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "            for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "                ind.accuracy = fit[1]\n",
    "                ind.raw_loss = fit[2]\n",
    "                ind.train_accuracy = fit[3]\n",
    "                ind.val_accuracy = fit[4]\n",
    "                ind.train_loss = fit[5]\n",
    "                ind.val_loss = fit[6]\n",
    "        \n",
    "        # Enregistrer les statistiques\n",
    "        record = stats.compile(population)\n",
    "        logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
    "        print(logbook.stream)\n",
    "        \n",
    "        hall_of_fame.update(population)\n",
    "        \n",
    "        best_ind = tools.selBest(population, 1)[0]\n",
    "        best_accuracy_per_gen.append(best_ind.accuracy)\n",
    "        best_loss_per_gen.append(best_ind.fitness.values[0])\n",
    "    \n",
    "        train_accuracies.append(best_ind.train_accuracy)\n",
    "        val_accuracies.append(best_ind.val_accuracy)\n",
    "        train_losses.append(best_ind.train_loss)\n",
    "        val_losses.append(best_ind.val_loss)\n",
    "\n",
    "        # Afficher le meilleur individu de chaque génération\n",
    "        print(f\"Generation {gen}: Best Individual = {best_ind}\")\n",
    "        \n",
    "        # Arrêter si un modèle avec 1 couche atteint une accuracy de 98%\n",
    "        if best_ind.accuracy >= 0.98 and len(decode_individual(best_ind)[0]) == 1:\n",
    "            print(\"Stopping early as a single layer model reached 98% accuracy.\")\n",
    "            break\n",
    "\n",
    "    # Affichage du meilleur individu\n",
    "    best_individual = hall_of_fame[0]\n",
    "    display_best_individual(best_individual, X_train.shape[1])\n",
    "\n",
    "    best_structure, best_weights = decode_individual(best_individual)\n",
    "    print('Best Individual Structure:', best_structure)\n",
    "    print('Fitness (Loss, Num Layers, Total Neurons):', best_individual.fitness.values)\n",
    "    print('Accuracy of the Best Model:', best_individual.accuracy)\n",
    "    print('Raw Loss:', best_individual.raw_loss)\n",
    "    \n",
    "    # Calcul de l'accuracy du meilleur modèle au cours des époques\n",
    "    best_model_layers = best_structure\n",
    "    best_model = Sequential()\n",
    "    best_model.add(Input(shape=(X_train.shape[1],)))\n",
    "    for neurons in best_model_layers:\n",
    "        best_model.add(Dense(neurons, activation='relu'))\n",
    "    best_model.add(Dense(output_dim, activation='softmax'))\n",
    "    best_model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "    history = best_model.fit(X_train, y_train, epochs=20, verbose=0, validation_data=(X_test, y_test))\n",
    "    loss, accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Final Accuracy of the Best Model:', accuracy)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs for Best Model')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy over Epochs for Best Model')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(best_loss_per_gen, label='Best Loss per Generation')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Best Loss per Generation')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(best_accuracy_per_gen, label='Best Accuracy per Generation')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Best Accuracy per Generation')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Fonction pour afficher la structure et les poids du meilleur individu\n",
    "def display_best_individual(best_individual, input_dim):\n",
    "    structure, weights = decode_individual(best_individual)\n",
    "    \n",
    "    print(\"Best Individual Structure:\")\n",
    "    print(structure)\n",
    "    \n",
    "    print(\"\\nBest Individual Weights:\")\n",
    "    weight_idx = 0\n",
    "    input_dim_layer = input_dim\n",
    "    for i, neurons in enumerate(structure):\n",
    "        output_dim_layer = neurons\n",
    "        layer_weights = np.array(weights[weight_idx:weight_idx + input_dim_layer * output_dim_layer]).reshape(input_dim_layer, output_dim_layer)\n",
    "        print(f\"Layer {i + 1} - Weights:\\n{layer_weights}\")\n",
    "        weight_idx += input_dim_layer * output_dim_layer\n",
    "        input_dim_layer = output_dim_layer\n",
    "\n",
    "    print(\"\\nBest Individual Biases:\")\n",
    "    for i, neurons in enumerate(structure):\n",
    "        biases = weights[weight_idx:weight_idx + neurons]\n",
    "        print(f\"Layer {i + 1} - Biases:\\n{biases}\")\n",
    "        weight_idx += neurons\n",
    "\n",
    "# Exemple d'utilisation\n",
    "filepath = cla_gen(lignes = 10000, colonnes = 10, nb_classes = 30)\n",
    "run_genetic_algorithm(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089346d3",
   "metadata": {},
   "source": [
    "REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef7e39",
   "metadata": {},
   "source": [
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36602ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Charger les données depuis un fichier CSV\n",
    "def load_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    X = data.iloc[:, :-1].values  # Toutes les colonnes sauf la dernière sont des caractéristiques\n",
    "    y = data.iloc[:, -1].values  # La dernière colonne est la cible\n",
    "    return X, y\n",
    "\n",
    "# Prétraiter les données\n",
    "def preprocess_data(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, X.shape[1], 1\n",
    "\n",
    "# Créer le toolbox pour les individus\n",
    "def create_individual_toolbox(input_dim, output_dim):\n",
    "    if 'FitnessMin' not in creator.__dict__:\n",
    "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "    if 'Individual' not in creator.__dict__:\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "    \n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_int\", np.random.randint, 5, 12)\n",
    "    toolbox.register(\"attr_float\", np.random.uniform, -1, 1)\n",
    "    \n",
    "    def generate_individual(num_layers=None):\n",
    "        if num_layers is None:\n",
    "            num_layers = np.random.randint(1, 5)  # Nombre de couches entre 1 et 4\n",
    "        structure = [np.random.randint(5, 12) for _ in range(num_layers)]\n",
    "        total_weights = sum([structure[i-1] * structure[i] if i > 0 else input_dim * structure[i] for i in range(num_layers)])\n",
    "        weights = [toolbox.attr_float() for _ in range(total_weights)]\n",
    "        individual = structure + weights\n",
    "        return creator.Individual(individual)\n",
    "    \n",
    "    def generate_diverse_population(n):\n",
    "        population = []\n",
    "        seen_structures = set()\n",
    "        while len(population) < n:\n",
    "            num_layers = np.random.randint(1, 5)\n",
    "            individual = generate_individual(num_layers=num_layers)\n",
    "            structure, _ = decode_individual(individual)\n",
    "            structure_tuple = tuple(structure)\n",
    "            if structure_tuple not in seen_structures:\n",
    "                population.append(individual)\n",
    "                seen_structures.add(structure_tuple)\n",
    "        return population\n",
    "\n",
    "    toolbox.register(\"individual\", generate_individual)\n",
    "    toolbox.register(\"population\", generate_diverse_population)\n",
    "\n",
    "    return toolbox\n",
    "\n",
    "# Fonction pour extraire la structure et les poids d'un individu\n",
    "def decode_individual(individual):\n",
    "    num_layers = len([x for x in individual if isinstance(x, int)])\n",
    "    structure = individual[:num_layers]\n",
    "    weights = individual[num_layers:]\n",
    "    return structure, weights\n",
    "\n",
    "# Fonction d'évaluation de l'erreur du modèle\n",
    "def eval_nn(individual, input_dim, output_dim, X_train, y_train, X_test, y_test):\n",
    "    structure, weights = decode_individual(individual)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "    total_neurons = 0\n",
    "    weight_idx = 0\n",
    "    for neurons in structure:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        total_neurons += neurons\n",
    "        output_dim_layer = neurons\n",
    "        if weight_idx + input_dim * output_dim_layer <= len(weights):\n",
    "            layer_weights = np.array(weights[weight_idx:weight_idx + input_dim * output_dim_layer]).reshape(input_dim, output_dim_layer)\n",
    "            weight_idx += input_dim * output_dim_layer\n",
    "            input_dim = output_dim_layer\n",
    "        else:\n",
    "            return (float('inf'),), float('inf'), [], [], []  # Eviter les erreurs d'index\n",
    "    model.add(Dense(output_dim))\n",
    "    \n",
    "    weight_idx = 0\n",
    "    input_dim = X_train.shape[1]\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Dense):\n",
    "            output_dim_layer = layer.units\n",
    "            if weight_idx + input_dim * output_dim_layer <= len(weights):\n",
    "                layer_weights = np.array(weights[weight_idx:weight_idx + input_dim * output_dim_layer]).reshape(input_dim, output_dim_layer)\n",
    "                weight_idx += input_dim * output_dim_layer\n",
    "                layer.set_weights([layer_weights, np.zeros(output_dim_layer)])\n",
    "                input_dim = output_dim_layer\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(0.01))\n",
    "    history = model.fit(X_train, y_train, epochs=20, verbose=0, validation_data=(X_test, y_test))\n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return (loss,), loss, history.history['loss'], history.history['val_loss']\n",
    "\n",
    "# Mutation des individus\n",
    "def mutate_individual(individual):\n",
    "    structure, weights = decode_individual(individual)\n",
    "    if np.random.random() < 0.5:\n",
    "        for i in range(len(structure)):\n",
    "            if np.random.random() < 0.2:\n",
    "                structure[i] = np.random.randint(5, 12)\n",
    "    if np.random.random() < 0.5:\n",
    "        weights = tools.mutPolynomialBounded(weights, low=-1, up=1, eta=0.1, indpb=0.2)[0]\n",
    "    individual[:] = structure + weights\n",
    "    return individual,\n",
    "\n",
    "# Exécution de l'algorithme génétique\n",
    "def run_genetic_algorithm(filepath, n_population=4, n_generations=10):\n",
    "    if isinstance(filepath, str):\n",
    "        X, y = load_data(filepath)\n",
    "    else:\n",
    "        X = filepath\n",
    "        y = X[X.columns[-1]]\n",
    "        X.drop(X.columns[-1], axis=1, inplace=True)\n",
    "        for column in X.columns:\n",
    "            if X[column].dtype == 'object' or X[column].dtype == 'str':\n",
    "                X[column] = tokenizator(X[column])\n",
    "    X_train, X_test, y_train, y_test, input_dim, output_dim = preprocess_data(X, y)\n",
    "    \n",
    "    toolbox = create_individual_toolbox(input_dim, output_dim)\n",
    "    toolbox.register(\"evaluate\", eval_nn, input_dim=input_dim, output_dim=output_dim, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", mutate_individual)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    \n",
    "    population = toolbox.population(n=n_population)\n",
    "    \n",
    "    # Évaluer tous les individus de la population initiale\n",
    "    fitnesses = list(map(toolbox.evaluate, population))\n",
    "    for ind, fit in zip(population, fitnesses):\n",
    "        ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "        ind.raw_loss = fit[1]\n",
    "        ind.train_loss = fit[2]\n",
    "        ind.val_loss = fit[3]\n",
    "    \n",
    "    hall_of_fame = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", lambda values: np.min([val[0] for val in values]))\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = [\"gen\", \"evals\"] + stats.fields\n",
    "    \n",
    "    best_loss_per_gen = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for gen in range(n_generations):\n",
    "        # Garder le meilleur individu de la génération précédente\n",
    "        if hall_of_fame:\n",
    "            population.append(toolbox.clone(hall_of_fame[0]))\n",
    "        \n",
    "        # Sélectionner les meilleurs individus\n",
    "        best_individuals = tools.selBest(population, k=int(0.1 * len(population)))\n",
    "        # Sélectionner quelques mauvais individus\n",
    "        worst_individuals = tools.selWorst(population, k=int(0.1 * len(population)))\n",
    "        # Sélectionner les individus restants\n",
    "        remaining_individuals = tools.selTournament(population, k=len(population) - len(best_individuals) - len(worst_individuals), tournsize=3)\n",
    "        \n",
    "        offspring = best_individuals + worst_individuals + remaining_individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if np.random.random() < 0.5:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        for mutant in offspring:\n",
    "            if np.random.random() < 0.2:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "        \n",
    "        # Évaluer les individus invalides\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "            ind.raw_loss = fit[1]\n",
    "            ind.train_loss = fit[2]\n",
    "            ind.val_loss = fit[3]\n",
    "        \n",
    "        # Ajouter quelques nouveaux individus aléatoires pour la diversité\n",
    "        new_random_individuals = [toolbox.individual(num_layers=np.random.randint(1, 5)) for _ in range(int(0.1 * len(population)))]\n",
    "        population[:] = offspring + new_random_individuals\n",
    "        \n",
    "        # S'assurer qu'il y a des individus avec 1 et 2 couches\n",
    "        has_one_layer = any(len(decode_individual(ind)[0]) == 1 for ind in population)\n",
    "        has_two_layers = any(len(decode_individual(ind)[0]) == 2 for ind in population)\n",
    "        if not has_one_layer:\n",
    "            population.append(toolbox.individual(num_layers=1))\n",
    "        if not has_two_layers:\n",
    "            population.append(toolbox.individual(num_layers=2))\n",
    "        \n",
    "        # Trier la population du meilleur au moins bon\n",
    "        valid_population = [ind for ind in population if ind.fitness.valid]\n",
    "        population.sort(key=lambda ind: ind.fitness.values[0] if ind.fitness.valid else float('inf'))\n",
    "        \n",
    "        # S'assurer que tous les individus ont des valeurs de fitness valides\n",
    "        invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "        if invalid_ind:\n",
    "            fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "            for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "                ind.raw_loss = fit[1]\n",
    "                ind.train_loss = fit[2]\n",
    "                ind.val_loss = fit[3]\n",
    "        \n",
    "        # Enregistrer les statistiques\n",
    "        record = stats.compile(population)\n",
    "        logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
    "        print(logbook.stream)\n",
    "        \n",
    "        hall_of_fame.update(population)\n",
    "        \n",
    "        best_ind = tools.selBest(population, 1)[0]\n",
    "        best_loss_per_gen.append(best_ind.fitness.values[0])\n",
    "    \n",
    "        train_losses.append(best_ind.train_loss)\n",
    "        val_losses.append(best_ind.val_loss)\n",
    "\n",
    "        # Afficher le meilleur individu de chaque génération\n",
    "        print(f\"Generation {gen}: Best Individual = {best_ind}\")\n",
    "\n",
    "    # Affichage du meilleur individu\n",
    "    best_individual = hall_of_fame[0]\n",
    "    display_best_individual(best_individual, X_train.shape[1])\n",
    "\n",
    "    best_structure, best_weights = decode_individual(best_individual)\n",
    "    print('Best Individual Structure:', best_structure)\n",
    "    print('Fitness (Loss, Num Layers, Total Neurons):', best_individual.fitness.values)\n",
    "    print('Raw Loss:', best_individual.raw_loss)\n",
    "    \n",
    "    # Calcul de la perte du meilleur modèle au cours des époques\n",
    "    best_model_layers = best_structure\n",
    "    best_model = Sequential()\n",
    "    best_model.add(Input(shape=(X_train.shape[1],)))\n",
    "    for neurons in best_model_layers:\n",
    "        best_model.add(Dense(neurons, activation='relu'))\n",
    "    best_model.add(Dense(output_dim))\n",
    "    best_model.compile(loss='mean_squared_error', optimizer=Adam(0.01))\n",
    "    history = best_model.fit(X_train, y_train, epochs=20, verbose=0, validation_data=(X_test, y_test))\n",
    "    loss = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Final Loss of the Best Model:', loss)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs for Best Model')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(best_loss_per_gen, label='Best Loss per Generation')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Best Loss per Generation')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Fonction pour afficher la structure et les poids du meilleur individu\n",
    "def display_best_individual(best_individual, input_dim):\n",
    "    structure, weights = decode_individual(best_individual)\n",
    "    \n",
    "    print(\"Best Individual Structure:\")\n",
    "    print(structure)\n",
    "    \n",
    "    print(\"\\nBest Individual Weights:\")\n",
    "    weight_idx = 0\n",
    "    input_dim_layer = input_dim\n",
    "    for i, neurons in enumerate(structure):\n",
    "        output_dim_layer = neurons\n",
    "        layer_weights = np.array(weights[weight_idx:weight_idx + input_dim_layer * output_dim_layer]).reshape(input_dim_layer, output_dim_layer)\n",
    "        print(f\"Layer {i + 1} - Weights:\\n{layer_weights}\")\n",
    "        weight_idx += input_dim_layer * output_dim_layer\n",
    "        input_dim_layer = output_dim_layer\n",
    "\n",
    "    print(\"\\nBest Individual Biases:\")\n",
    "    for i, neurons in enumerate(structure):\n",
    "        biases = weights[weight_idx:weight_idx + neurons]\n",
    "        print(f\"Layer {i + 1} - Biases:\\n{biases}\")\n",
    "        weight_idx += neurons\n",
    "\n",
    "# Exemple d'utilisation\n",
    "filepath = 'housing.csv'  # Remplacez par le chemin de votre fichier CSV\n",
    "run_genetic_algorithm(filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f484bf",
   "metadata": {},
   "source": [
    "Concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176bc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Charger les données depuis un fichier CSV\n",
    "def load_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    X = data.iloc[:, :-1].values  # Toutes les colonnes sauf la dernière sont des caractéristiques\n",
    "    y = data.iloc[:, -1].values  # La dernière colonne est la cible\n",
    "    return X, y\n",
    "\n",
    "# Prétraiter les données\n",
    "def preprocess_data(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test, X.shape[1], 1\n",
    "\n",
    "# Créer le toolbox pour les individus\n",
    "def create_individual_toolbox(input_dim, output_dim):\n",
    "    if 'FitnessMin' not in creator.__dict__:\n",
    "        creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "    if 'Individual' not in creator.__dict__:\n",
    "        creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "    \n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"attr_int\", np.random.randint, 5, 12)\n",
    "    toolbox.register(\"attr_float\", np.random.uniform, -1, 1)\n",
    "    \n",
    "    def generate_individual(num_layers=None):\n",
    "        if num_layers is None:\n",
    "            num_layers = np.random.randint(1, 5)  # Nombre de couches entre 1 et 4\n",
    "        structure = [np.random.randint(5, 12) for _ in range(num_layers)]\n",
    "        total_weights = sum([structure[i-1] * structure[i] if i > 0 else input_dim * structure[i] for i in range(num_layers)])\n",
    "        weights = [toolbox.attr_float() for _ in range(total_weights)]\n",
    "        individual = structure + weights\n",
    "        return creator.Individual(individual)\n",
    "    \n",
    "    def generate_diverse_population(n):\n",
    "        population = []\n",
    "        seen_structures = set()\n",
    "        while len(population) < n:\n",
    "            num_layers = np.random.randint(1, 5)\n",
    "            individual = generate_individual(num_layers=num_layers)\n",
    "            structure, _ = decode_individual(individual)\n",
    "            structure_tuple = tuple(structure)\n",
    "            if structure_tuple not in seen_structures:\n",
    "                population.append(individual)\n",
    "                seen_structures.add(structure_tuple)\n",
    "        return population\n",
    "\n",
    "    toolbox.register(\"individual\", generate_individual)\n",
    "    toolbox.register(\"population\", generate_diverse_population)\n",
    "\n",
    "    return toolbox\n",
    "\n",
    "# Fonction pour extraire la structure et les poids d'un individu\n",
    "def decode_individual(individual):\n",
    "    num_layers = len([x for x in individual if isinstance(x, int)])\n",
    "    structure = individual[:num_layers]\n",
    "    weights = individual[num_layers:]\n",
    "    return structure, weights\n",
    "\n",
    "# Fonction d'évaluation de l'erreur du modèle\n",
    "def eval_nn(individual, input_dim, output_dim, X_train, y_train, X_test, y_test):\n",
    "    structure, weights = decode_individual(individual)\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))\n",
    "    total_neurons = 0\n",
    "    weight_idx = 0\n",
    "    for neurons in structure:\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        total_neurons += neurons\n",
    "        output_dim_layer = neurons\n",
    "        if weight_idx + input_dim * output_dim_layer <= len(weights):\n",
    "            layer_weights = np.array(weights[weight_idx:weight_idx + input_dim * output_dim_layer]).reshape(input_dim, output_dim_layer)\n",
    "            weight_idx += input_dim * output_dim_layer\n",
    "            input_dim = output_dim_layer\n",
    "        else:\n",
    "            return (float('inf'),), float('inf'), [], [], []  # Eviter les erreurs d'index\n",
    "    model.add(Dense(output_dim))\n",
    "    \n",
    "    weight_idx = 0\n",
    "    input_dim = X_train.shape[1]\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, Dense):\n",
    "            output_dim_layer = layer.units\n",
    "            if weight_idx + input_dim * output_dim_layer <= len(weights):\n",
    "                layer_weights = np.array(weights[weight_idx:weight_idx + input_dim * output_dim_layer]).reshape(input_dim, output_dim_layer)\n",
    "                weight_idx += input_dim * output_dim_layer\n",
    "                layer.set_weights([layer_weights, np.zeros(output_dim_layer)])\n",
    "                input_dim = output_dim_layer\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(0.01))\n",
    "    history = model.fit(X_train, y_train, epochs=20, verbose=0, validation_data=(X_test, y_test))\n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return (loss,), loss, history.history['loss'], history.history['val_loss']\n",
    "\n",
    "# Mutation des individus\n",
    "def mutate_individual(individual):\n",
    "    structure, weights = decode_individual(individual)\n",
    "    if np.random.random() < 0.5:\n",
    "        for i in range(len(structure)):\n",
    "            if np.random.random() < 0.2:\n",
    "                structure[i] = np.random.randint(5, 12)\n",
    "    if np.random.random() < 0.5:\n",
    "        weights = tools.mutPolynomialBounded(weights, low=-1, up=1, eta=0.1, indpb=0.2)[0]\n",
    "    individual[:] = structure + weights\n",
    "    return individual,\n",
    "\n",
    "# Exécution de l'algorithme génétique\n",
    "def run_genetic_algorithm(filepath, n_population=4, n_generations=10):\n",
    "    if isinstance(filepath, str):\n",
    "        X, y = load_data(filepath)\n",
    "    else:\n",
    "        X = filepath\n",
    "        y = X[X.columns[-1]]\n",
    "        X.drop(X.columns[-1], axis=1, inplace=True)\n",
    "        for column in X.columns:\n",
    "            if X[column].dtype == 'object' or X[column].dtype == 'str':\n",
    "                X[column] = tokenizator(X[column])\n",
    "    X_train, X_test, y_train, y_test, input_dim, output_dim = preprocess_data(X, y)\n",
    "    \n",
    "    toolbox = create_individual_toolbox(input_dim, output_dim)\n",
    "    toolbox.register(\"evaluate\", eval_nn, input_dim=input_dim, output_dim=output_dim, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "    toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "    toolbox.register(\"mutate\", mutate_individual)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    \n",
    "    population = toolbox.population(n=n_population)\n",
    "    \n",
    "    # Évaluer tous les individus de la population initiale\n",
    "    fitnesses = list(map(toolbox.evaluate, population))\n",
    "    for ind, fit in zip(population, fitnesses):\n",
    "        ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "        ind.raw_loss = fit[1]\n",
    "        ind.train_loss = fit[2]\n",
    "        ind.val_loss = fit[3]\n",
    "    \n",
    "    hall_of_fame = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", lambda values: np.min([val[0] for val in values]))\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = [\"gen\", \"evals\"] + stats.fields\n",
    "    \n",
    "    best_loss_per_gen = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for gen in range(n_generations):\n",
    "        # Garder le meilleur individu de la génération précédente\n",
    "        if hall_of_fame:\n",
    "            population.append(toolbox.clone(hall_of_fame[0]))\n",
    "        \n",
    "        # Sélectionner les meilleurs individus\n",
    "        best_individuals = tools.selBest(population, k=int(0.1 * len(population)))\n",
    "        # Sélectionner quelques mauvais individus\n",
    "        worst_individuals = tools.selWorst(population, k=int(0.1 * len(population)))\n",
    "        # Sélectionner les individus restants\n",
    "        remaining_individuals = tools.selTournament(population, k=len(population) - len(best_individuals) - len(worst_individuals), tournsize=3)\n",
    "        \n",
    "        offspring = best_individuals + worst_individuals + remaining_individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "            if np.random.random() < 0.5:\n",
    "                toolbox.mate(child1, child2)\n",
    "                del child1.fitness.values\n",
    "                del child2.fitness.values\n",
    "        for mutant in offspring:\n",
    "            if np.random.random() < 0.2:\n",
    "                toolbox.mutate(mutant)\n",
    "                del mutant.fitness.values\n",
    "        \n",
    "        # Évaluer les individus invalides\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "            ind.raw_loss = fit[1]\n",
    "            ind.train_loss = fit[2]\n",
    "            ind.val_loss = fit[3]\n",
    "        \n",
    "        # Ajouter quelques nouveaux individus aléatoires pour la diversité\n",
    "        new_random_individuals = [toolbox.individual(num_layers=np.random.randint(1, 5)) for _ in range(int(0.1 * len(population)))]\n",
    "        population[:] = offspring + new_random_individuals\n",
    "        \n",
    "        # S'assurer qu'il y a des individus avec 1 et 2 couches\n",
    "        has_one_layer = any(len(decode_individual(ind)[0]) == 1 for ind in population)\n",
    "        has_two_layers = any(len(decode_individual(ind)[0]) == 2 for ind in population)\n",
    "        if not has_one_layer:\n",
    "            population.append(toolbox.individual(num_layers=1))\n",
    "        if not has_two_layers:\n",
    "            population.append(toolbox.individual(num_layers=2))\n",
    "        \n",
    "        # Trier la population du meilleur au moins bon\n",
    "        valid_population = [ind for ind in population if ind.fitness.valid]\n",
    "        population.sort(key=lambda ind: ind.fitness.values[0] if ind.fitness.valid else float('inf'))\n",
    "        \n",
    "        # S'assurer que tous les individus ont des valeurs de fitness valides\n",
    "        invalid_ind = [ind for ind in population if not ind.fitness.valid]\n",
    "        if invalid_ind:\n",
    "            fitnesses = map(toolbox.evaluate, invalid_ind)\n",
    "            for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                ind.fitness.values = fit[0]  # Prendre uniquement le premier élément\n",
    "                ind.raw_loss = fit[1]\n",
    "                ind.train_loss = fit[2]\n",
    "                ind.val_loss = fit[3]\n",
    "        \n",
    "        # Enregistrer les statistiques\n",
    "        record = stats.compile(population)\n",
    "        logbook.record(gen=gen, evals=len(invalid_ind), **record)\n",
    "        print(logbook.stream)\n",
    "        \n",
    "        hall_of_fame.update(population)\n",
    "        \n",
    "        best_ind = tools.selBest(population, 1)[0]\n",
    "        best_loss_per_gen.append(best_ind.fitness.values[0])\n",
    "    \n",
    "        train_losses.append(best_ind.train_loss)\n",
    "        val_losses.append(best_ind.val_loss)\n",
    "\n",
    "        # Afficher le meilleur individu de chaque génération\n",
    "        print(f\"Generation {gen}: Best Individual = {best_ind}\")\n",
    "\n",
    "    # Affichage du meilleur individu\n",
    "    best_individual = hall_of_fame[0]\n",
    "    display_best_individual(best_individual, X_train.shape[1])\n",
    "\n",
    "    best_structure, best_weights = decode_individual(best_individual)\n",
    "    print('Best Individual Structure:', best_structure)\n",
    "    print('Fitness (Loss, Num Layers, Total Neurons):', best_individual.fitness.values)\n",
    "    print('Raw Loss:', best_individual.raw_loss)\n",
    "    \n",
    "    # Calcul de la perte du meilleur modèle au cours des époques\n",
    "    best_model_layers = best_structure\n",
    "    best_model = Sequential()\n",
    "    best_model.add(Input(shape=(X_train.shape[1],)))\n",
    "    for neurons in best_model_layers:\n",
    "        best_model.add(Dense(neurons, activation='relu'))\n",
    "    best_model.add(Dense(output_dim))\n",
    "    best_model.compile(loss='mean_squared_error', optimizer=Adam(0.01))\n",
    "    history = best_model.fit(X_train, y_train, epochs=20, verbose=0, validation_data=(X_test, y_test))\n",
    "    loss = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Final Loss of the Best Model:', loss)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs for Best Model')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(best_loss_per_gen, label='Best Loss per Generation')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Best Loss per Generation')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Fonction pour afficher la structure et les poids du meilleur individu\n",
    "def display_best_individual(best_individual, input_dim):\n",
    "    structure, weights = decode_individual(best_individual)\n",
    "    \n",
    "    print(\"Best Individual Structure:\")\n",
    "    print(structure)\n",
    "    \n",
    "    print(\"\\nBest Individual Weights:\")\n",
    "    weight_idx = 0\n",
    "    input_dim_layer = input_dim\n",
    "    for i, neurons in enumerate(structure):\n",
    "        output_dim_layer = neurons\n",
    "        layer_weights = np.array(weights[weight_idx:weight_idx + input_dim_layer * output_dim_layer]).reshape(input_dim_layer, output_dim_layer)\n",
    "        print(f\"Layer {i + 1} - Weights:\\n{layer_weights}\")\n",
    "        weight_idx += input_dim_layer * output_dim_layer\n",
    "        input_dim_layer = output_dim_layer\n",
    "\n",
    "    print(\"\\nBest Individual Biases:\")\n",
    "    for i, neurons in enumerate(structure):\n",
    "        biases = weights[weight_idx:weight_idx + neurons]\n",
    "        print(f\"Layer {i + 1} - Biases:\\n{biases}\")\n",
    "        weight_idx += neurons\n",
    "\n",
    "# Exemple d'utilisation\n",
    "filepath = reg_gen(lignes = 10000, colonnes = 10)  # Remplacez par le chemin de votre fichier CSV\n",
    "run_genetic_algorithm(filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
